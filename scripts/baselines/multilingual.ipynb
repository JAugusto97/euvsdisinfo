{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_NAME=\"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_NAME)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/euvsdisinfo.csv\")\n",
    "df[\"text\"] = df[\"article_title\"].fillna(\"\") + \" \" + df[\"article_text\"]\n",
    "df[\"label\"] = df[\"class\"].apply(lambda x: 0 if x == \"support\" else 1)\n",
    "languages = df[\"article_language\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"text\", \"label\", \"article_language\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for seed in range(10):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.3, random_state=seed, stratify=df[\"article_language\"])\n",
    "    language_weights = {language: len(test_df[test_df[\"article_language\"] == language])/len(test_df) for language in test_df[\"article_language\"].unique()}\n",
    "\n",
    "    print(language_weights)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\", \"article_language\"])\n",
    "    # define the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_NAME, num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results-multi',\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        push_to_hub=False,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        save_strategy=\"epoch\",\n",
    "        seed=seed,\n",
    "        data_seed=seed\n",
    "    )\n",
    "\n",
    "    # define the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # predict\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    preds = predictions.predictions.argmax(-1)\n",
    "    labels = predictions.label_ids\n",
    "    langs = test_dataset[\"article_language\"]\n",
    "\n",
    "    preds_df = pd.DataFrame({\"preds\": preds, \"labels\": labels, \"langs\": langs})\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    precision_negative = evaluate.load(\"precision\", pos_label=0)\n",
    "    precision_positive = evaluate.load(\"precision\", pos_label=1)\n",
    "    recall_negative = evaluate.load(\"recall\", pos_label=0)\n",
    "    recall_positive = evaluate.load(\"recall\", pos_label=1)\n",
    "    results = {\"language\": [], \"f1\": [], \"weight\": [], \"seed\": []}\n",
    "\n",
    "    weighted_f1 = 0\n",
    "    for language in set(langs):\n",
    "        df_lang = preds_df[preds_df[\"langs\"] == language]\n",
    "        l = df_lang[\"labels\"].tolist()\n",
    "        p = df_lang[\"preds\"].tolist()\n",
    "        f1_score = f1.compute(predictions=p, references=l, average=\"macro\")[\"f1\"]\n",
    "        p_neg = precision_negative.compute(predictions=p, references=l, average=\"binary\")[\"precision\"]\n",
    "        p_pos = precision_positive.compute(predictions=p, references=l, average=\"binary\")[\"precision\"]\n",
    "        r_neg = recall_negative.compute(predictions=p, references=l, average=\"binary\")[\"recall\"]\n",
    "        r_pos = recall_positive.compute(predictions=p, references=l, average=\"binary\")[\"recall\"]\n",
    "\n",
    "        weight = len(df_lang) / len(preds_df)\n",
    "        weighted_f1 += f1_score * weight\n",
    "\n",
    "        print(language, len(df_lang), f1_score, p_neg, p_pos, r_neg, r_pos)\n",
    "        results[\"language\"].append(language)\n",
    "        results[\"f1\"].append(f1_score)\n",
    "        results[\"weight\"].append(weight)\n",
    "        results[\"seed\"].append(seed)\n",
    "\n",
    "    # avg_f1 /= len(set(langs))\n",
    "    print(\"Weighted F1\", weighted_f1)\n",
    "    final_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\n",
    "    \"English\",\n",
    "    \"Russian\",\n",
    "    \"French\",\n",
    "    \"German\",\n",
    "    \"Spanish\",\n",
    "    \"Polish\",\n",
    "    \"Czech\",\n",
    "    \"Azerbaijani\",\n",
    "    \"Lithuanian\",\n",
    "    \"Italian\",\n",
    "    \"Romanian\",\n",
    "    \"Finnish\",\n",
    "    \"Bulgarian\",\n",
    "    \"Croatian\"\n",
    "]\n",
    "\n",
    "results = pd.concat([pd.DataFrame.from_dict(r) for r in final_results])\n",
    "results = results.groupby([\"language\"]).agg({\"f1\": [\"mean\", \"std\"], \"weight\": [\"mean\"]}).reset_index().set_index(\"language\").loc[languages]\n",
    "weighted_f1 = (results[\"f1\"][\"mean\"].values * results[\"weight\"][\"mean\"].values).sum()\n",
    "avg_f1 = results[\"f1\"][\"mean\"].mean()\n",
    "results.loc[\"Weighted F1\"] = [weighted_f1, \"\", \"\"]\n",
    "results.loc[\"Avg. F1\"] = [avg_f1, \"\", \"\"]\n",
    "results.columns = [\"F1 Mean\", \"F1 Std\", \"Weight\"]\n",
    "results = results.reset_index()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euvsdisinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
