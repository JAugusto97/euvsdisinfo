{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_debunks():\n",
    "    \"\"\"Loads a dataframe containing the debunk articles from EuvsDisinfo.\"\"\"\n",
    "    path_raw = os.path.join(\"../../data/raw/\")\n",
    "    dfs = []\n",
    "    for fname in os.listdir(path_raw):\n",
    "        if fname.endswith(\".json\"):\n",
    "            fpath = os.path.join(path_raw, fname)\n",
    "\n",
    "            df = pd.read_json(fpath)\n",
    "            df = pd.DataFrame(df[\"disinfoCases\"].tolist())\n",
    "            dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df.drop_duplicates(subset=\"id\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df = df.rename({\"id\": \"debunk_id\"}, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/euvsdisinfo_full.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debunks_df = load_debunks()\n",
    "debunks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_string):\n",
    "    # Using string.punctuation to get a string of all ASCII punctuation characters\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # Removing punctuation using translate method\n",
    "    result_string = input_string.translate(translator)\n",
    "    \n",
    "    return result_string\n",
    "\n",
    "def html_to_text(html_string):\n",
    "    soup = BeautifulSoup(html_string, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "def get_top_ngrams(text, n, top_n):\n",
    "    n_grams = get_ngrams(text, n)\n",
    "    freq_dist = FreqDist(n_grams)\n",
    "    return freq_dist.most_common(top_n)\n",
    "\n",
    "input_text = \" \".join(debunks_df[\"disproof\"].str.casefold().apply(html_to_text).apply(remove_punctuation))\n",
    "\n",
    "top_unigrams = get_top_ngrams(input_text, 1, 50)\n",
    "top_bigrams = get_top_ngrams(input_text, 2, 50)\n",
    "top_trigrams = get_top_ngrams(input_text, 3, 50)\n",
    "top_4grams = get_top_ngrams(input_text, 4, 50)\n",
    "\n",
    "print(\"Top 50 Unigrams:\")\n",
    "print(top_unigrams)\n",
    "\n",
    "print(\"\\nTop 50 Bigrams:\")\n",
    "print(top_bigrams)\n",
    "\n",
    "print(\"\\nTop 50 Trigrams:\")\n",
    "print(top_trigrams)\n",
    "\n",
    "print(\"\\nTop 50 4-grams:\")\n",
    "print(top_4grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_4grams, columns=[\"4gram\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_trigrams, columns=[\"3gram\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rules = [\n",
    "    \"recurring prokremlin disinformation narrative\",\n",
    "    \"prokremlin disinformation narrative about\",\n",
    "    \"disinformation narrative about the\",\n",
    "    \"see other examples of\",\n",
    "    \"a recurring prokremlin disinformation\",\n",
    "    \"this is a recurring\",\n",
    "    \"disinformation cases alleging that\",\n",
    "    \"similar cases claiming that\",\n",
    "    \"prokremlin disinformation narratives about\",\n",
    "    \"recurring prokremlin disinformation narratives\",\n",
    "    \"read more about the\",\n",
    "    \"read similar cases claiming\",\n",
    "    \"is a recurring prokremlin\",\n",
    "    \"other examples of similar\",\n",
    "    \"recurring prokremlin narrative about\",\n",
    "    \"a recurring prokremlin narrative\",\n",
    "    \"a recurring disinformation narrative\",\n",
    "    \"earlier disinformation cases alleging\",\n",
    "    \"see earlier disinformation cases\",\n",
    "    \"disinformation narratives about the\",\n",
    "    \"recurring prokremlin disinformation\",\n",
    "    \"prokremlin disinformation narrative\",\n",
    "    \"disinformation narrative about\",\n",
    "    \"a recurring prokremlin\",\n",
    "    \"see other examples\",\n",
    "    \"prokremlin disinformation narratives\",\n",
    "    \"recurring prokremlin narrative\",\n",
    "    \"other examples of\",\n",
    "    \"disinformation narratives about\",\n",
    "    \"is a recurring\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the disproof text into sentences\n",
    "debunks_df['sentences'] = debunks_df['disproof'].apply(sent_tokenize)\n",
    "debunks_df['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = debunks_df[\"sentences\"].explode().tolist()\n",
    "\n",
    "filtered_sentences = []\n",
    "for sentence in sentences:\n",
    "    clean_sentence = remove_punctuation(html_to_text(sentence.lower()))\n",
    "    for rule in filter_rules:\n",
    "        if rule in clean_sentence:\n",
    "            filtered_sentences.append(sentence)\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "urls = [BeautifulSoup(sentence, \"html.parser\").find_all(\"a\") for sentence in filtered_sentences]\n",
    "urls = [url.get(\"href\") for lurl in urls for url in lurl]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_urls = df[df['article_url'].isin(urls)]\n",
    "matched_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_urls[\"article_publisher\"].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustworthy = [\n",
    "    \"bbc\",\n",
    "    \"reuters\",\n",
    "    \"the guardian\",\n",
    "    \"dw.com\",\n",
    "    \"radiofreeeurope/radioliberty\",\n",
    "    \"washington post\",\n",
    "    \"cnn\",\n",
    "    \"ap news\",\n",
    "    \"euronews\",\n",
    "    \"politico\",\n",
    "    \"npr\",\n",
    "    \"new york times\",\n",
    "    \"france 24\",\n",
    "    \"polygraph.info\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_df = matched_urls[~matched_urls[\"article_publisher\"].isin(trustworthy)]\n",
    "to_remove_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euvsdisinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
