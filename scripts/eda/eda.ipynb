{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/euvsdisinfo_full.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"debunk_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"debunk_id\")[[\"class\"]].value_counts().reset_index().groupby(\"class\")[\"count\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"debunk_id\")[[\"class\"]].value_counts().reset_index().groupby(\"class\")[\"count\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"debunk_id\")[[\"class\"]].value_counts().reset_index().groupby(\"class\")[\"count\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"article_text\"].str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"keywords\"].dropna().apply(lambda x: len(x.split(\",\"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"article_language\", data=df, hue=\"class\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with the total number of articles per language, and the distribution of classes for each language. Insert 0 if there are no articles of a certain class for a certain language\n",
    "languages = df[\"article_language\"].unique()\n",
    "classes = df[\"class\"].unique().tolist()\n",
    "total_articles = []\n",
    "class_distributions = []\n",
    "\n",
    "for language in languages:\n",
    "    total_articles.append(len(df[df[\"article_language\"] == language]))\n",
    "    for class_ in classes:\n",
    "        class_distributions.append(\n",
    "            len(df[(df[\"article_language\"] == language) & (df[\"class\"] == class_)])\n",
    "        )\n",
    "\n",
    "class_distributions = np.array(class_distributions).reshape(\n",
    "    len(languages), len(classes)\n",
    ")\n",
    "\n",
    "distributions_df = pd.DataFrame({\"total\": class_distributions.sum(1), \"disinformation\": class_distributions[:,1], \"support\": class_distributions[:,0]}, index=languages\n",
    ").sort_values(\"total\", ascending=False)\n",
    "distributions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions_df[\"total\"].quantile([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of supporting articles per language\n",
    "(df[df[\"class\"] == \"support\"][\"article_language\"].value_counts() / df[\"article_language\"].value_counts()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(df[\"debunk_date\"], format=\"%d-%m-%Y\")\n",
    "print(dates.min(),\"||\", dates.max())\n",
    "period = relativedelta(dates.max(), dates.min())\n",
    "print(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"debunk_date\"] = pd.to_datetime(df[\"debunk_date\"], format=\"%d-%m-%Y\")\n",
    "\n",
    "# extract year and quarter from debunk_date column\n",
    "df['year'] = pd.to_datetime(df['debunk_date'], format='%d-%m-%Y').dt.year\n",
    "df['quarter'] = pd.to_datetime(df['debunk_date'], format='%d-%m-%Y').dt.quarter\n",
    "\n",
    "# create a new column that combines year and quarter information\n",
    "df['year_quarter'] = df['year'].astype(str) + '-Q' + df['quarter'].astype(str)\n",
    "\n",
    "# group data by year_quarter and class, and plot stacked distribution.\n",
    "grouped_df = df.groupby(['year_quarter', 'class']).size().unstack().fillna(0)\n",
    "\n",
    "# reindex the DataFrame to include missing dates with a count of zero\n",
    "all_dates = pd.date_range(start=df['debunk_date'].min(), end=df['debunk_date'].max(), freq='Q')\n",
    "grouped_df = grouped_df.reindex([d.split(\"-\")[0] + \"-Q\" + str(int(d.split(\"-\")[1])//4 +1) for d in  all_dates.strftime('%Y-%m')])\n",
    "\n",
    "grouped_df.plot(kind='bar', stacked=True)\n",
    "plt.xlabel('Year and Quarter')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Year and Quarter and Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = df[df[\"class\"] == \"disinformation\"]\n",
    "kw_ot_df = df_fake[[\"keywords\", \"debunk_date\"]].dropna()\n",
    "kw_ot_df[\"keywords\"] = kw_ot_df[\"keywords\"].apply(lambda x: x.split(\",\"))\n",
    "kw_ot_df = kw_ot_df.explode(\"keywords\")\n",
    "kw_ot_df[\"keywords\"] = kw_ot_df[\"keywords\"].str.strip()\n",
    "kw_ot_df[\"keywords\"] = kw_ot_df[\"keywords\"].str.capitalize()\n",
    "print(\"Unique Topics\", kw_ot_df[\"keywords\"].nunique())\n",
    "print(kw_ot_df[\"keywords\"].value_counts().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine_kw = [\n",
    "    \"War in ukraine\",\n",
    "    \"Crimea\",\n",
    "    \"Invasion of ukraine\",\n",
    "    \"Donbas\",\n",
    "    \"Illegal annexation\"\n",
    "    \"Ukraine\",\n",
    "    \"Eastern ukraine\",\n",
    "    \"War crimes\",\n",
    "    \"Ukrainian statehood\"\n",
    "]\n",
    "\n",
    "covid_kw = [\n",
    "    \"Coronavirus\",\n",
    "    \"Vaccination\",\n",
    "    \"Biological weapons\",\n",
    "    \"Chemical weapons/attack\",\n",
    "    \"Conspiracy theory\",\n",
    "    \"Laboratory\",\n",
    "    \"Virus / bacteria threat\"\n",
    "]\n",
    "\n",
    "west_kw = [\n",
    "    \"West\",\n",
    "    \"Nato\",\n",
    "    \"European union\",\n",
    "    \"International law\",\n",
    "    \"Us presence in europe\",\n",
    "    \"Eu/nato enlargement\",\n",
    "    \"Europe\",\n",
    "    \"United nations\",\n",
    "]\n",
    "\n",
    "russia_kw = [\n",
    "    \"Anti-russian\",\n",
    "    \"Russophobia\",\n",
    "    \"Alexei navalny\",\n",
    "    \"Encircling russia\",\n",
    "    \"Destabilising russia\",\n",
    "    \"Diplomacy with russia\",\n",
    "    \"Ussr\",\n",
    "    \"Russian world\"\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_ukraine = {\n",
    "    \"War in ukraine\": \"War in Ukraine\",\n",
    "    \"Crimea\": \"Crimea\",\n",
    "    \"Invasion of ukraine\": \"Invasion of Ukraine\",\n",
    "    \"Donbas\": \"Donbas\",\n",
    "    \"Illegal annexation\": \"Illegal annexation\",\n",
    "    \"Ukraine\": \"Ukraine\",\n",
    "    \"Eastern ukraine\": \"Eastern Ukraine\",\n",
    "    \"War crimes\": \"War crimes\",\n",
    "    \"Ukrainian statehood\": \"Ukrainian statehood\"\n",
    "}\n",
    "\n",
    "map_west = {\n",
    "    \"West\": \"West\",\n",
    "    \"Nato\": \"NATO\",\n",
    "    \"European union\": \"European Union\",\n",
    "    \"International law\": \"International law\",\n",
    "    \"Us presence in europe\": \"US presence in Europe\",\n",
    "    \"Eu/nato enlargement\": \"EU/NATO enlargement\",\n",
    "    \"Europe\": \"Europe\",\n",
    "    \"United nations\": \"United Nations\"\n",
    "}\n",
    "\n",
    "map_covid = {\n",
    "    \"Coronavirus\": \"Coronavirus\",\n",
    "    \"Vaccination\": \"Vaccination\",\n",
    "    \"Biological weapons\": \"Biological weapons\",\n",
    "    \"Chemical weapons/attack\": \"Chemical weapons/attack\",\n",
    "    \"Conspiracy theory\": \"Conspiracy theory\",\n",
    "    \"Laboratory\": \"Laboratory\",\n",
    "    \"Virus / bacteria threat\": \"Virus/bacteria threat\"\n",
    "}\n",
    "\n",
    "map_russia = {\n",
    "    \"Anti-russian\": \"Anti-Russian\",\n",
    "    \"Russophobia\": \"Russophobia\",\n",
    "    \"Alexei navalny\": \"Alexei Navalny\",\n",
    "    \"Encircling russia\": \"Encircling Russia\",\n",
    "    \"Destabilising russia\": \"Destabilising Russia\",\n",
    "    \"Diplomacy with russia\": \"Diplomacy with Russia\",\n",
    "    \"Ussr\": \"USSR\",\n",
    "    \"Russian world\": \"Russian world\"\n",
    "}\n",
    "\n",
    "mapping = {k: v for d in [map_ukraine, map_west, map_covid, map_russia] for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_quarters = sorted(df[\"year_quarter\"].unique())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(10, 19))\n",
    "\n",
    "big_fontsize = 20\n",
    "medium_fontsize = 17\n",
    "\n",
    "# Create a shared y-axis label\n",
    "fig.text(-0.04, 0.5, \"# Disinformation Articles\", va='center', rotation='vertical', fontsize=big_fontsize)\n",
    "\n",
    "for i, top_kw in enumerate([covid_kw, west_kw, russia_kw, ukraine_kw]):\n",
    "    df_fake = df[df[\"class\"] == \"disinformation\"]\n",
    "    kw_ot_df = df_fake[[\"keywords\", \"debunk_date\"]].dropna()\n",
    "    kw_ot_df[\"keywords\"] = kw_ot_df[\"keywords\"].apply(lambda x: x.split(\",\"))\n",
    "    kw_ot_df = kw_ot_df.explode(\"keywords\")\n",
    "    kw_ot_df[\"keywords\"] = kw_ot_df[\"keywords\"].str.strip()\n",
    "    kw_ot_df[\"keywords\"] = kw_ot_df[\"keywords\"].str.capitalize()\n",
    "    kw_ot_df = kw_ot_df[kw_ot_df[\"keywords\"].isin(top_kw)]\n",
    "    kw_ot_df[\"keywords\"] = kw_ot_df[\"keywords\"].map(mapping)\n",
    "\n",
    "    kw_ot_df[\"debunk_date\"] = pd.to_datetime(kw_ot_df[\"debunk_date\"], format=\"%Y-%m-%d\")\n",
    "    kw_ot_df[\"debunk_date\"] = kw_ot_df[\"debunk_date\"].dt.year.astype(str) + \"-Q\" + kw_ot_df[\"debunk_date\"].dt.quarter.astype(str)\n",
    "    kw_ot_df = kw_ot_df[kw_ot_df[\"debunk_date\"] != \"2023-Q3\"]  # remove last quarter of 2023 because it is incomplete\n",
    "    kw_ot_df.set_index(\"debunk_date\", inplace=True)\n",
    "    kw_ot_df= kw_ot_df.pivot_table(columns=\"keywords\", aggfunc=\"size\", index=\"debunk_date\").fillna(0)\n",
    "    kw_ot_df = kw_ot_df.reindex(year_quarters, fill_value=0)\n",
    "\n",
    "    ax = axes[i]  # Get the correct subplot axes\n",
    "\n",
    "    kw_ot_df.plot(ax=ax)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    title = \"COVID-19\" if i == 0 else \"West\" if i == 1 else \"Russia\" if i == 2 else \"Ukraine\"\n",
    "    ax.set_title(title, fontsize=big_fontsize)\n",
    "    ax.title.set_weight('bold')\n",
    "    # Set x-axis tick positions\n",
    "    ax.set_xticks(range(len(kw_ot_df.index)))\n",
    "\n",
    "    # Set x-axis tick labels with increased font size\n",
    "    ax.set_xticklabels(kw_ot_df.index, rotation=90, fontsize=medium_fontsize)\n",
    "\n",
    "    # Set y-axis tick labels with increased font size\n",
    "    ax.tick_params(axis='y', labelsize=medium_fontsize)\n",
    "\n",
    "    if top_kw == covid_kw:\n",
    "        ax.axvline(x=kw_ot_df.index.tolist().index(\"2020-Q1\"), color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "        # Annotate the vertical line as \"Pandemic\"\n",
    "        ax.annotate(\n",
    "            \"Start of COVID-19 Pandemic\",\n",
    "            xy=(kw_ot_df.index.tolist().index(\"2020-Q1\"), 250),\n",
    "            xytext=(kw_ot_df.index.tolist().index(\"2020-Q1\") +0.3 , 270),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"black\"),\n",
    "            color=\"black\",\n",
    "            fontsize=18,\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        ax.axvline(x=kw_ot_df.index.tolist().index(\"2022-Q1\"), color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "        # Annotate the vertical line as \"Ukraine Invasion\"\n",
    "        ax.annotate(\n",
    "            \"Invasion of Ukraine\",\n",
    "            xy=(kw_ot_df.index.tolist().index(\"2022-Q1\"), 250),\n",
    "            xytext=(kw_ot_df.index.tolist().index(\"2022-Q1\") - 14, 270),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"black\"),\n",
    "            color=\"black\",\n",
    "            fontsize=20,\n",
    "        )   \n",
    "\n",
    "    # Show vertical grid lines\n",
    "    # ax.xaxis.grid(True)\n",
    "\n",
    "    # Show xticks only for the last plot\n",
    "    if i != 3:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "# Adjust the spacing between subplots and make the spacing for xticks the same for all axes\n",
    "plt.tight_layout(pad=1.0, h_pad=1.0)\n",
    "\n",
    "# Reduce the size of the legend box\n",
    "for ax in axes:\n",
    "    ax.legend(prop={'size': 15})\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_ylim(0, 300)\n",
    "\n",
    "# make all plots have the same number of xticks\n",
    "for ax in axes:\n",
    "    ax.set_xticks(range(len(kw_ot_df.index)))\n",
    "    ax.set_xlim(0, len(kw_ot_df.index))\n",
    "# save as pdf\n",
    "plt.savefig(\"keywords_over_time.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams = df[(df[\"year_quarter\"] == \"2021-Q3\") & (df[\"class\"] == \"disinformation\")]\n",
    "df_ngrams[\"article_language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df.groupby(['article_publisher', 'class']).size().reset_index(name='counts')\n",
    "topn = 25\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 15))\n",
    "\n",
    "misinfo_df = class_counts[class_counts['class'] == 'disinformation']\n",
    "misinfo_df = misinfo_df.sort_values(by='counts', ascending=True).tail(topn)\n",
    "ax1.barh(misinfo_df['article_publisher'], misinfo_df['counts'], color='red')\n",
    "ax1.set_title(\"Disinformation\", fontsize=23)\n",
    "ax1.title.set_weight('bold')\n",
    "ax1.set_yticklabels(misinfo_df['article_publisher'], fontsize=20)\n",
    "ax1.tick_params(axis='x', labelsize=20) \n",
    "\n",
    "support_df = class_counts[class_counts['class'] == 'support']\n",
    "support_df = support_df.sort_values(by='counts', ascending=True).tail(topn)\n",
    "ax2.barh(support_df['article_publisher'], support_df['counts'], color='green')\n",
    "ax2.set_title(\"Trustworthy\", fontsize=23)\n",
    "ax2.title.set_weight('bold')\n",
    "ax2.set_yticklabels(support_df['article_publisher'], fontsize=20)\n",
    "ax2.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "plt.tight_layout(pad=1.0, h_pad=1.0)\n",
    "plt.savefig(\"top_publishers.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df.groupby(['article_publisher', 'class']).size().reset_index(name='counts')\n",
    "misinfo_df = class_counts[class_counts['class'] == 'disinformation']\n",
    "counts = misinfo_df[misinfo_df['article_publisher'].str.contains(\"ria\")][\"counts\"].sum()\n",
    "total = misinfo_df[\"counts\"].sum()\n",
    "\n",
    "print(counts/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts[class_counts['class'] == 'support'][\"article_publisher\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinfo_df[\"counts\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"article_publisher\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"class\"] == \"disinformation\"][\"article_publisher\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"class\"] == \"support\"][\"article_publisher\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_misinfo = df[df['class'] == 'disinformation']\n",
    "df_misinfo[\"keywords\"].dropna().apply(lambda x: x.split(\",\")).explode().str.strip().value_counts().head(50).plot(kind='bar', figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = df_misinfo[\"keywords\"].dropna().apply(lambda x: x.split(\",\")).explode().str.strip().value_counts().head(15)\n",
    "top_publishers = df['article_language'].value_counts().head(15).index\n",
    "topics_publishers = {}\n",
    "\n",
    "for row in df_misinfo[[\"article_language\", \"keywords\"]].dropna().apply(lambda x: {x[\"article_language\"]: [k.strip() for k in x[\"keywords\"].split(\",\")]}, axis=1):\n",
    "    for publisher, topics in row.items():\n",
    "        for topic in topics:\n",
    "            if topic in top_topics:\n",
    "                if topic not in topics_publishers:\n",
    "                    topics_publishers[topic] = []\n",
    "                else:\n",
    "                    topics_publishers[topic].append(publisher)\n",
    "\n",
    "# count the number of occurences of each publisher for each topic\n",
    "topic_publisher_counts = {}\n",
    "for topic, publishers in topics_publishers.items():\n",
    "    topic_publisher_counts[topic] = {}\n",
    "    for publisher in publishers:\n",
    "        if publisher not in topic_publisher_counts[topic]:\n",
    "            topic_publisher_counts[topic][publisher] = 0\n",
    "        topic_publisher_counts[topic][publisher] += 1\n",
    "# transform the counts into percentages\n",
    "topic_publisher_percentages = {}\n",
    "for topic, publisher_counts in topic_publisher_counts.items():\n",
    "    total = sum(publisher_counts.values())\n",
    "    topic_publisher_percentages[topic] = {publisher: count / total for publisher, count in publisher_counts.items()}\n",
    "    \n",
    "# keep the top 5 publishers for each topic, and aggregate the rest into a new key called \"Others\"\n",
    "topic_publisher_percentages_top5 = {}\n",
    "for topic, publisher_percentages in topic_publisher_percentages.items():\n",
    "    topic_publisher_percentages_top5[topic] = {}\n",
    "    top5_publishers = sorted(publisher_percentages.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for publisher, percentage in top5_publishers:\n",
    "        topic_publisher_percentages_top5[topic][publisher] = percentage\n",
    "    topic_publisher_percentages_top5[topic][\"Others\"] = sum(publisher_percentages.values()) - sum([percentage for publisher, percentage in top5_publishers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = df_misinfo[[\"article_language\", \"keywords\"]].dropna().apply(lambda x: {x[\"article_language\"]: [k.strip() for k in x[\"keywords\"].split(\",\")]}, axis=1)\n",
    "k = 8\n",
    "top_keywords = df_misinfo[\"keywords\"].dropna().apply(lambda x: x.split(\",\")).explode().str.strip().value_counts()[:k].keys()\n",
    "top_languages = df_misinfo[\"article_language\"].value_counts().head(9).keys()\n",
    "counts = {}\n",
    "for d in dicts:\n",
    "    for lang, keywords in d.items():\n",
    "        if lang in top_languages:\n",
    "            if lang not in counts:\n",
    "                counts[lang] = {}\n",
    "            for keyword in keywords:\n",
    "                if keyword in top_keywords:\n",
    "                    if keyword not in counts[lang]:\n",
    "                        counts[lang][keyword] = 0\n",
    "                    counts[lang][keyword] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 10 pie charts for each language. keep the colors consistent for each keyword across different charts\n",
    "colors = sns.color_palette(\"Paired\", k+1)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(30, 15))\n",
    "for i, (lang, keyword_counts) in enumerate(counts.items()):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    ax.set_title(lang, fontsize=25)  # Increase the font size of the title\n",
    "    # make the title bold\n",
    "    ax.title.set_weight('bold')\n",
    "\n",
    "    # sort the keyword_counts dictionary by keys\n",
    "    keyword_counts = dict(sorted(keyword_counts.items()))\n",
    "    \n",
    "    ax.pie(keyword_counts.values(), labels=keyword_counts.keys(), autopct='%1.1f%%', colors=[colors[top_keywords.get_loc(keyword)] for keyword in keyword_counts.keys()], textprops={'fontsize': 25})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_keywords_per_language.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "top_15_languages = df[\"article_language\"].value_counts().head(15).index.tolist()\n",
    "df_top_15_languages = df[df[\"article_language\"].isin(top_15_languages)]\n",
    "# d[\"language\"] = df_top_15_languages.groupby('article_language')['keywords'].apply(lambda x: x.str.split(',').explode().str.strip().value_counts().index[0]).index.tolist()\n",
    "for i in range(3):\n",
    "    d[f\"top{i+1}\"] = df_top_15_languages.groupby('article_language')['keywords'].apply(lambda x: x.str.split(',').explode().str.strip().value_counts().index[i]).values.tolist()\n",
    "\n",
    "pd.DataFrame(d, index = top_15_languages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euvsdisinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
